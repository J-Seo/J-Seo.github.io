---
layout: post
title: "Research on Recent Quality Estimation (DC 2021)"
date: 2021-07-28
Journal: Journal of Digital Convergence
---

**Authors**

- Sugyeong Eo, Chanjun Park, Hyeonseok Moon, **Jaehyung Seo**, Heuiseok Lim

**Abstract**

Quality estimation (QE) can evaluate the quality of machine translation output even for those who do not know the target language, and its high utilization highlights the need for QE. QE shared task is held every year at Conference on Machine Translation (WMT), and recently, researches applying Pretrained Language Model (PLM) are mainly being conducted. In this paper, we conduct a survey on the QE task and research trends, and we summarize the features of PLM. In addition, we used a multilingual BART model that has not yet been utilized and performed comparative analysis with the existing studies such as XLM, multilingual BERT, and XLM-RoBERTa. As a result of the experiment, we confirmed which PLM was most effective when applied to QE, and saw the possibility of applying the multilingual BART model to the QE task.

Check out the [This Link][DOI] for more info on our paper. 

[DOI]: https://doi.org/10.15207/JKCS.2021.12.7.037
[jekyll-gh]: https://github.com/jekyll/jekyll
