---
layout: post
title: "LimaCost: Data Valuation for Instruction Tuning of Large Language Models"
date: 2025-08-20
image: "https://raw.githubusercontent.com/J-Seo/J-Seo.github.io/main/assets/img/emnlp2025.png"
Journal: EMNLP 2025 - findings
authors: Hyeonseok Moon, Jaehyung Seo, Seonmin Koo, Jinsung Kim, Youngkyoung Ham, Jiwon Moon, Heuiseok Lim*
categories: outstanding
---
**Authors**
- Hyeonseok Moon, **Jaehyung Seo**, Seonmin Koo, Jinsung Kim, Young-kyoung Ham, jiwon moon, Heuiseok Lim<sup>*</sup>

**Abstract**

Instruction tuning(IT) is an effective approach for aligning large language models (LLMs) with human intentions. There is ongoing discourse regarding the data quality for IT. As an effort to find the robust criteria of data quality for IT, we introduce LimaCost, a data quality measure that exhibits a strong correlation with model performance. LimaCost utilizes LIMA dataset, which effectiveness in IT has already been validated by several previous works. LimaCost then estimates the value of a given data by estimating how many LIMA data points might be needed to approximate its gradient. Our experiments reveal that LimaCost enables effective data selection that derive high alignment performance. We demonstrate that selecting data based on high LimaCost proves to be more effective than existing data selection strategies.

Check out the [This Link][DOI] for more info on our paper

[DOI]: https://aclanthology.org/2025.findings-emnlp.688/

