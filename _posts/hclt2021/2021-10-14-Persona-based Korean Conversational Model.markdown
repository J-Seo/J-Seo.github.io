---
layout: post
title: "Persona-based Korean Conversational Model (HCLT 2021)"
date: 2021-10-14
Journal: Annual Conference on Human and Language Technology 
---
**Annual Conference on Human and Language Technology** 

**Authors**

- Yoonna Jang, Jungwoo Lim, Yuna Hur, Kisu Yang, Chanjun Park, **Jaehyung Seo**, Seungjun Lee, Heuiseok Lim

**Abstract**

대화형 에이전트가 일관성 없는 답변, 재미 없는 답변을 하는 문제를 해결하기 위하여 최근 페르소나 기반의 대화 분야의 연구가 활발히 진행되고 있다. 그러나 한국어로 구축된 페르소나 대화 데이터는 아직 구축되지 않은 상황이다. 이에 본 연구에서는 영어 원본 데이터에서 한국어로 번역된 데이터를 활용하여 최초의 페르소나 기반 한국어 대화 모델을 제안한다. 전처리를 통하여 번역 품질을 향상시킨 데이터에 사전 학습 된 한국어 모델인 KoBERT와 KoELECTRA를 미세조정(fine-tuning) 시킴으로써 모델에게 주어진 페르소나와 대화 맥락을 고려하여 올바른 답변을 선택하는 모델을 학습한다. 실험 결과 KoELECTRA-base 모델이 가장 높은 성능을 보이는 것을 확인하였으며, 단순하게 사용자의 발화만을 주는 것 보다 이전 대화 이력이 추가적으로 주어졌을 때 더 좋은 성능을 보이는 것을 확인할 수 있었다.

Check out the [This Link][DOI] for more info on our paper. 

[DOI]: https://koreascience.kr/article/CFKO202130060718834.page
[jekyll-gh]: https://github.com/jekyll/jekyll
