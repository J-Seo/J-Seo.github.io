---
layout: post
title: "Empirical Analysis of Noising Scheme based Synthetic Data Generation for Automatic Post-editing"
date: 2022-06-15
Journal: LREC 2022
categories: outstanding
authors: Hyeonseok Moon, Chanjun Park, Seolhwa Lee, Jaehyung Seo, Jungseob Lee, Sugyeong Eo, Heuiseok Lim*
image: "https://raw.githubusercontent.com/J-Seo/J-Seo.github.io/main/assets/img/lrec2022.png"
---
**Authors**
- Hyeonseok Moon, Chanjun Park, Seolhwa Lee, **Jaehyung Seo**, Jungseob Lee, Sugyeong Eo, Heuiseok Lim<sup>*</sup>

**Abstract**
Automatic post-editing (APE) refers to a research field that aims to automatically correct errors included in the translation sentences derived by the machine translation system. This study has several limitations, considering the data acquisition, because there is no official dataset for most language pairs. Moreover, the amount of data is restricted even for language pairs in which official data has been released, such as WMT. To solve this problem and promote universal APE research regardless of APE data existence, this study proposes a method for automatically generating APE data based on a noising scheme from a parallel corpus. Particularly, we propose a human mimicking errors-based noising scheme that considers a practical correction process at the human level. We propose a precise inspection to attain high performance, and we derived the optimal noising schemes that show substantial effectiveness. Through these, we also demonstrate that depending on the type of noise, the noising scheme-based APE data generation may lead to inferior performance. In addition, we propose a dynamic noise injection strategy that enables the acquisition of a robust error correction capability and demonstrated its effectiveness by comparative analysis. This study enables obtaining a high performance APE model without human-generated data and can promote universal APE research for all language pairs targeting English.

Check out the [This Link][DOI] for more info on our paper

[DOI]: https://aclanthology.org/2022.lrec-1.93

